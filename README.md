# Local_Helper

#### 介绍
支持本地大模型的可视化应用
本项目是一个轻量级本地大模型调用框架
大家只需要拉取模型，或者将训练后的模型导入到ollama，直接使用该应用即可实现本地模型的调用，从此告别排队！
项目已经开源至gitee、github
github: https://github.com/fanxt0218/Local_Helper
如果您觉得该开源项目对您有帮助，请给作者点个star，谢谢！
更详细的发开介绍 --> https://blog.csdn.net/2402_84949062?spm=1011.2266.3001.5343  <--请访问我的主页

#### 软件架构◪
Java语言基于Springboot框架开发


#### 安装教程〄

1.  将项目克隆到本地
2.  启动ollama服务，ollama serve(默认启动)
3.  点击启动即可

#### 使用说明☼

1.  确保本机已安装ollama服务且ollama已经存在模型
2.  配置文件中更改ip信息，连接数据库
3.  执行doc目录下的local_helper_init.sql执行脚本
4.  ---> 先启动mcp-server模块、再启动local_helper_client模块 <----
5.  项目启动后浏览器访问1618端口
6.  右上角下拉框选择本地的不同大模型进行使用
7.  在输入框输入信息点击发送即可使用,同样可以添加附件
8.  页面左下角可以设置各种参数

### 注意 ###

如果要使用mcp服务请先在mcp-server模块中配置相关配置
通过转发端口使用ollama服务速度可能较慢，尤其是参数较大的模型
由于项目仍在初期阶段，许多功能仍未完善
项目会不断改进和维护，希望大家多多支持♥

#### 参与贡献✉

1.  该项目由gitee快乐哦呜所属
2.  如若在使用过程中遇到bug，请在gitee或github提出您的建议
3.  想要参与对项目的开发和完善，请联系我
4.  作者学生党,望大家多多支持
5.  开发者CSDN账号：Fanxt_Ja
